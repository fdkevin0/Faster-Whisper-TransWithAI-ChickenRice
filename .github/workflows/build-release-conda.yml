name: Build and Release with Conda

on:
  push:
    branches: [ main ]
    tags:
      - 'v*'
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      release_version:
        description: 'Release version (e.g., v1.0.0)'
        required: false
        type: string
      include_chickenrice:
        description: 'Include Chickenrice model in releases'
        required: false
        type: boolean
        default: false

jobs:
  build-windows:
    name: Build Windows - CUDA ${{ matrix.cuda }} ${{ matrix.model_variant }}
    runs-on: windows-latest
    defaults:
      run:
        shell: bash -el {0}  # Important for conda activation on Windows
    strategy:
      matrix:
        include:
          # CUDA 11.8 versions
          - cuda: "11.8"
            env_file: "environment-cuda118.yml"
            env_name: "faster-whisper-cu118"
            artifact_suffix: "cu118"
            model_variant: "base"
            hf_model: ""
          - cuda: "11.8"
            env_file: "environment-cuda118.yml"
            env_name: "faster-whisper-cu118"
            artifact_suffix: "cu118-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
          # CUDA 12.2 versions
          - cuda: "12.2"
            env_file: "environment-cuda122.yml"
            env_name: "faster-whisper-cu122"
            artifact_suffix: "cu122"
            model_variant: "base"
            hf_model: ""
          - cuda: "12.2"
            env_file: "environment-cuda122.yml"
            env_name: "faster-whisper-cu122"
            artifact_suffix: "cu122-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
          # CUDA 12.8 versions
          - cuda: "12.8"
            env_file: "environment-cuda128.yml"
            env_name: "faster-whisper-cu128"
            artifact_suffix: "cu128"
            model_variant: "base"
            hf_model: ""
          - cuda: "12.8"
            env_file: "environment-cuda128.yml"
            env_name: "faster-whisper-cu128"
            artifact_suffix: "cu128-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure stdout buffering
      run: |
        # Enable line buffering for better performance with large outputs
        echo "Configuring buffering for improved CI performance..."
        echo "PYTHONUNBUFFERED=1" >> $GITHUB_ENV
        # For shell commands, we'll use stdbuf where needed
        echo "Buffering configuration complete."

    - name: Cache conda packages
      uses: actions/cache@v4
      id: conda-cache
      env:
        CACHE_NUMBER: 1  # Increment to invalidate cache
      with:
        path: |
          ~/conda_pkgs_dir
        key: ${{ runner.os }}-conda-pkgs-${{ matrix.env_name }}-${{ matrix.cuda }}-${{ env.CACHE_NUMBER }}-${{ hashFiles(matrix.env_file) }}
        restore-keys: |
          ${{ runner.os }}-conda-pkgs-${{ matrix.env_name }}-${{ matrix.cuda }}-${{ env.CACHE_NUMBER }}-
          ${{ runner.os }}-conda-pkgs-${{ matrix.env_name }}-${{ matrix.cuda }}-


    - name: Setup Miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniforge-version: latest
        auto-update-conda: true
        environment-file: ${{ matrix.env_file }}
        activate-environment: ${{ matrix.env_name }}
        show-channel-urls: true
        use-only-tar-bz2: true
        use-mamba: true  # Use mamba for faster dependency resolution
        # Add conda-pkgs-dir to use cached packages
        pkgs-dirs: ~/conda_pkgs_dir
        python-version: "3.10"

    - name: Force reinstall ctranslate2 for CUDA 11.8
      if: matrix.cuda == '11.8'
      run: |
        echo "Force reinstalling ctranslate2==3.24.0 for CUDA 11.8 compatibility..."
        pip install --force-reinstall ctranslate2==3.24.0 numpy==1.26.4
        echo "ctranslate2 reinstalled successfully"
        python -c "import ctranslate2; print(f'CTranslate2 version: {ctranslate2.__version__}')"

    - name: Fix onnxruntime CPU/GPU conflict
      run: |
        echo "Removing onnxruntime CPU version to avoid conflicts..."
        pip uninstall onnxruntime -y || true
        echo ""
        echo "Installing appropriate onnxruntime-gpu version for CUDA ${{ matrix.cuda }}..."
        if [ "${{ matrix.cuda }}" = "11.8" ]; then
          echo "Installing onnxruntime-gpu==1.18.0 for CUDA 11.8..."
          pip install onnxruntime-gpu==1.18.0
        elif [ "${{ matrix.cuda }}" = "12.2" ] || [ "${{ matrix.cuda }}" = "12.8" ]; then
          echo "Installing onnxruntime-gpu==1.20.2 for CUDA ${{ matrix.cuda }}..."
          pip install onnxruntime-gpu==1.20.2
        else
          echo "Installing onnxruntime-gpu>=1.17.0 for CUDA ${{ matrix.cuda }}..."
          pip install "onnxruntime-gpu>=1.17.0"
        fi
        echo ""
        echo "Verifying onnxruntime-gpu installation..."
        python -c "import onnxruntime as ort; print(f'ONNX Runtime version: {ort.__version__}'); print(f'Available providers: {ort.get_available_providers()}')" || echo "Note: GPU providers won't show on GitHub runners (no GPU)"

    - name: Report conda cache status
      run: |
        echo "Conda packages cache hit: ${{ steps.conda-cache.outputs.cache-hit }}"
        if [ "${{ steps.conda-cache.outputs.cache-hit }}" = "true" ]; then
          echo "Package cache was restored, installation should be faster"
        else
          echo "Package cache miss, downloading packages"
        fi
        echo ""
        echo "Conda environment location:"
        conda info --envs

    - name: Cache HuggingFace models
      if: matrix.hf_model != ''
      uses: actions/cache@v4
      with:
        # Only cache HuggingFace model subdirectories, not VAD models
        # The chickenrice model goes into models/whisper-large-v2-translate-zh-v0.2-st-ct2/
        path: |
          models/*/
          !models/*.onnx
          !models/*.json
        key: hf-model-${{ matrix.model_variant }}-${{ hashFiles('download_models.py') }}-${{ matrix.hf_model }}
        restore-keys: |
          hf-model-${{ matrix.model_variant }}-${{ hashFiles('download_models.py') }}-
          hf-model-${{ matrix.model_variant }}-

    - name: Display environment info
      run: |
        conda info
        conda list
        python --version
        python -c "import ctranslate2; print(f'CTranslate2 version: {ctranslate2.__version__}')"
        echo "Note: CUDA availability check skipped (no GPU on GitHub runners)"

    - name: Check cached models
      run: |
        echo "Checking for cached models..."
        if [ -d "models" ]; then
          echo "Models directory exists:"
          # Use buffered find instead of ls for better performance
          find models -maxdepth 1 -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null | head -20
          echo ""
          echo "Model subdirectories (HuggingFace models only, cached):"
          # Pre-calculate all sizes at once
          du -sh models/*/ 2>/dev/null | while read size dir; do
            echo "  - $(basename "$dir"): $size"
          done
          echo ""
          echo "Root model files (VAD models, not cached):"
          find models -maxdepth 1 \( -name "*.onnx" -o -name "*.json" \) -printf "%s %p\n" 2>/dev/null | \
            awk '{size=$1; $1=""; printf "  %s (%s)\n", $0, size}' || echo "  No VAD model files yet"
        else
          echo "Models directory does not exist yet"
        fi

    - name: Download models
      run: |
        python download_models.py ${{ matrix.hf_model }}
      continue-on-error: false

    - name: Verify downloaded models
      run: |
        echo "Model files after download:"
        echo ""
        echo "VAD models (not cached, always re-downloaded):"
        find models -maxdepth 1 \( -name "*.onnx" -o -name "*.json" \) -printf "  %p (%s bytes)\n" 2>/dev/null || echo "  No VAD model files found"
        echo ""
        if [ "${{ matrix.hf_model }}" != "" ]; then
          echo "HuggingFace models (cached):"
          # Pre-calculate all directory sizes
          du -sh models/*/ 2>/dev/null > /tmp/model_sizes.txt
          for dir in models/*/; do
            if [ -d "$dir" ]; then
              echo "  Directory: $(basename "$dir")"
              find "$dir" -maxdepth 1 -type f -printf "    %f (%s bytes)\n" 2>/dev/null | head -10
              size=$(grep "$dir" /tmp/model_sizes.txt | cut -f1)
              echo "  Total size: $size"
              echo ""
            fi
          done
        else
          echo "No HuggingFace models (base variant)"
        fi


    - name: Build with PyInstaller
      run: |
        echo "Using conda environment: $CONDA_DEFAULT_ENV"
        echo "Python path: $(which python)"
        echo "PyInstaller version:"
        python -m pip show pyinstaller
        export PYTHONPATH="${PYTHONPATH}:${PWD}/src"

        python build_windows.py

    - name: Copy models to distribution
      run: |
        echo "Copying models to distribution directory..."
        if [ -d "models" ]; then
          echo "Found models directory"
          echo "Contents of models directory:"
          find models -maxdepth 1 -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null
          echo ""

          # Create models directory in dist
          mkdir -p dist/faster_whisper_transwithai_chickenrice/models

          # Copy VAD model files (always included)
          echo "Copying VAD models..."
          cp models/*.onnx dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
          cp models/*.json dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true

          # Copy HuggingFace models if this is a chickenrice variant
          if [ "${{ matrix.model_variant }}" = "chickenrice" ]; then
            echo "Copying Chickenrice model..."
            for dir in models/*/; do
              if [ -d "$dir" ]; then
                model_name=$(basename "$dir")
                echo "  Copying model contents from: $model_name"
                # Copy the contents of the model directory, not the directory itself
                cp -r "$dir"* dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
                # Also copy hidden files if any exist
                cp -r "$dir".* dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
              fi
            done
          fi

          echo ""
          echo "Models in distribution:"
          find dist/faster_whisper_transwithai_chickenrice/models -maxdepth 1 -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null
          echo ""
          echo "Total distribution size:"
          du -sh dist/faster_whisper_transwithai_chickenrice/
        else
          echo "WARNING: Models directory not found!"
        fi

    - name: Copy batch files, configuration, and documentation
      run: |
        echo "Copying batch files, configuration, and documentation to distribution..."

        # Copy usage instructions
        if [ -f "ä½¿ç”¨è¯´æ˜Ž.txt" ]; then
          cp "ä½¿ç”¨è¯´æ˜Ž.txt" dist/faster_whisper_transwithai_chickenrice/
          echo "Copied: ä½¿ç”¨è¯´æ˜Ž.txt"
        fi

        # Copy release notes
        if [ -f "RELEASE_NOTES_CN.md" ]; then
          cp "RELEASE_NOTES_CN.md" dist/faster_whisper_transwithai_chickenrice/
          echo "Copied: RELEASE_NOTES_CN.md"
        fi

        # Copy generation config to root directory (for easy user editing)
        if [ -f "generation_config.json5" ]; then
          cp "generation_config.json5" dist/faster_whisper_transwithai_chickenrice/
          echo "Copied: generation_config.json5 to root directory"
        fi

        # Copy all batch files
        for bat_file in *.bat; do
          if [ -f "$bat_file" ]; then
            # Skip any build-related batch files
            if [[ "$bat_file" != *"build"* ]] && [[ "$bat_file" == "è¿è¡Œ"* ]]; then
              cp "$bat_file" dist/faster_whisper_transwithai_chickenrice/
              echo "Copied: $bat_file"
            fi
          fi
        done

        echo ""
        echo "Distribution contents:"
        find dist/faster_whisper_transwithai_chickenrice -maxdepth 1 \( -name "*.bat" -o -name "*.txt" -o -name "*.md" -o -name "*.json5" \) -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null || echo "No batch/text/config files found"

    - name: Test executable (CPU mode)
      shell: cmd /C CALL {0}
      run: |
        cd dist\faster_whisper_transwithai_chickenrice
        infer.exe --help
      continue-on-error: true

    - name: Upload artifact
      uses: actions/upload-artifact@v4
      with:
        name: faster_whisper_transwithai_windows_${{ matrix.artifact_suffix }}
        path: dist/faster_whisper_transwithai_chickenrice/
        retention-days: 30

    - name: List artifact directory structure
      run: |
        echo "========================================================================"
        echo "ðŸ“¦ ARTIFACT DIRECTORY STRUCTURE"
        echo "========================================================================"
        echo "Build variant: ${{ matrix.artifact_suffix }}"
        echo "CUDA version: ${{ matrix.cuda }}"
        echo "Model variant: ${{ matrix.model_variant }}"
        echo "------------------------------------------------------------------------"

        # Simple directory tree (depth limited to 3)
        echo "Directory structure:"
        find dist/faster_whisper_transwithai_chickenrice -type d -maxdepth 3 | \
          sed 's|[^/]*/|  |g' | \
          sed 's|^  |dist/|'

        echo ""
        echo "Total artifact size: $(du -sh dist/faster_whisper_transwithai_chickenrice/ | cut -f1)"
        echo "========================================================================"

  # Create the initial GitHub release
  create-release:
    name: Create GitHub Release
    needs: [build-windows]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write
    outputs:
      release_created: ${{ steps.create.outputs.release_created }}

    steps:
    - name: Checkout code for release notes
      uses: actions/checkout@v4
      with:
        sparse-checkout: |
          RELEASE_NOTES_CN.md
        sparse-checkout-cone-mode: false

    - name: Read release body
      id: read_body
      run: |
        if [ -f "RELEASE_NOTES_CN.md" ]; then
          echo 'body<<EOF' >> $GITHUB_OUTPUT
          cat RELEASE_NOTES_CN.md >> $GITHUB_OUTPUT
          echo 'EOF' >> $GITHUB_OUTPUT
        else
          echo 'body=Release created by GitHub Actions' >> $GITHUB_OUTPUT
        fi

    - name: Create empty placeholder file for initial release
      run: |
        echo "This release contains large binary files. Files are being uploaded..." > placeholder.txt

    - name: Create Release with placeholder
      id: create
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        name: ${{ github.ref_name }}
        body: ${{ steps.read_body.outputs.body }}
        draft: false
        prerelease: false
        files: placeholder.txt
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set output
      run: echo "release_created=true" >> $GITHUB_OUTPUT

  # Parallel upload jobs - each handles one artifact
  upload-cu118:
    name: Upload CUDA 11.8 Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu118
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 11.8 base variant..."
        # Using compression level 5 for faster builds (was level 9)
        # Level 5 provides good balance between speed and compression ratio
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu118.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu118.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu118.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu118-chickenrice:
    name: Upload CUDA 11.8 Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu118-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 11.8 chickenrice variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu118-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu118-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu118-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu122:
    name: Upload CUDA 12.2 Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu122
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.2 base variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu122.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu122.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu122.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu122-chickenrice:
    name: Upload CUDA 12.2 Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu122-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.2 chickenrice variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu122-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu122-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu122-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu128:
    name: Upload CUDA 12.8 Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu128
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.8 base variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu128.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu128.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu128.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu128-chickenrice:
    name: Upload CUDA 12.8 Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu128-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.8 chickenrice variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu128-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu128-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu128-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}